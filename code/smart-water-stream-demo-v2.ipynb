{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c35ebfd3-cc0d-4b30-9016-5f831d00afe5",
   "metadata": {},
   "source": [
    "# Data Pipeline - Batch and Stream Data to BigQuery with Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c41af1-c7f1-4c08-8ea4-032a1165799a",
   "metadata": {},
   "source": [
    "Prerequisites prior to running demo:\n",
    "1) Billing-enabled project\n",
    "2) API's enabled and necessary IAM role(s) for necessary services (e.g. Dataflow, BigQuery)\n",
    "3) GCS bucket for temporary and schema files\n",
    "4) BigQuery dataset (tables can / will be created if they don't already exist)\n",
    "5) Pub/Sub topic for streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0ebfd7-cab3-4b0d-b474-cd36f43635f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import apache_beam as beam\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "import google.auth\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8af55b-b72e-437c-8fa0-e4886d2a2a97",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataflow Batch - GCS Text Files to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b75dd2-0d94-475a-a755-de4b089afc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# function(s)\n",
    "\n",
    "def parse_json(line):\n",
    "    import json\n",
    "    record = json.loads(line)\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_batch(argv=None):\n",
    "    \"\"\"Build and run the pipeline.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd_mm_YY_H_M_S_ssssss - in this format so it can be used to create a unique BQ table\n",
    "    dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "    dt_string\n",
    "\n",
    "    SCHEMA = 'radioId:STRING,eventTime:DATETIME,reading:FLOAT,consumptionFlag:STRING,reverseFlowFlag:STRING,noFlowFlag:STRING,emptyPipeFlag:STRING,temperature:FLOAT,pressure:FLOAT,batteryStatusFlag:STRING'\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runner', required=False, default='DataflowRunner', help='Dataflow Runner - DataflowRunner or DirectRunner (local)')\n",
    "    parser.add_argument('--job_name', required=False, default='mgwaterdemobatch', help='Dataflow Job Name')\n",
    "    parser.add_argument('--project_id', required=False, default='mg-ce-demos', help='GCP Project ID')\n",
    "    parser.add_argument('--region', required=False, default='us-central1', help='GCP region for execution')\n",
    "    parser.add_argument('--dataset_name', required=False, default='smart_water_demo', help='Output BigQuery Dataset') \n",
    "    parser.add_argument('--table_name', required=False, default='smart_water_demo_data_batch_'+str(dt_string), help='Output BigQuery Table')\n",
    "    parser.add_argument('--input_data', required=False, default='gs://mg-ce-demos-bucket/water_data_stream_demo/output/*.json', help='input data (for batch only')\n",
    "\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    pipeline_args.extend([\n",
    "          '--runner=' + str(known_args.runner) # Change this to DataflowRunner to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--project=' + str(known_args.project_id) # Your project ID is required in order to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--staging_location=gs://mg-ce-demos-bucket/water_data_stream_demo/temp' # Your Google Cloud Storage path is required for staging local files.\n",
    "          ,'--temp_location=gs://mg-ce-demos-bucket/water_data_stream_demo/tmp' # Your Google Cloud Storage path is required for temporary files.\n",
    "          ,'--job_name=' + str(known_args.job_name) # Set project unique job name\n",
    "          ,'--region=' + str(known_args.region) # Set region if using DataflowRunner\n",
    "      ])\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    #pipeline_options.view_as(StandardOptions).streaming = True  # set to True if stream (remove if batch)\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        \n",
    "        # Read the JSON files in GCS into a PCollection.\n",
    "        events = ( p | beam.io.ReadFromText(known_args.input_data) )  #change to read files from GCS\n",
    "\n",
    "        # Tranform events\n",
    "        transformed = (events | beam.Map(parse_json))\n",
    "\n",
    "        # Persist to BigQuery\n",
    "        transformed | 'Write' >> beam.io.WriteToBigQuery(\n",
    "                            table=known_args.table_name,\n",
    "                            dataset=known_args.dataset_name,\n",
    "                            project=known_args.project_id,\n",
    "                            schema=SCHEMA,\n",
    "                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b1050c-f84e-45af-95fd-778d5ff64267",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 18 files in 0.07961249351501465 seconds.\n",
      "/root/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery.py:2138: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  is_streaming_pipeline = p.options.view_as(StandardOptions).streaming\n",
      "/root/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/io/gcp/bigquery_file_loads.py:1128: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.37.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp93txl4o5', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.37.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmp93txl4o5', 'apache-beam==2.37.0', '--no-deps', '--only-binary', ':all:', '--python-version', '37', '--implementation', 'cp', '--abi', 'cp37m', '--platform', 'manylinux1_x86_64']\n",
      "INFO:apache_beam.runners.portability.stager:Staging binary distribution of the SDK from PyPI: apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.7_sdk:2.37.0\n",
      "INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\n",
      "INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python37:2.37.0\" for Docker environment\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fbf1e267830> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fbf1e268050> ====================\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 18 files in 0.05775260925292969 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649696583.472144/pickled_main_session...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649696583.472144/pickled_main_session in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649696583.472144/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649696583.472144/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649696583.472144/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649696583.472144/apache_beam-2.37.0-cp37-cp37m-manylinux1_x86_64.whl in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649696583.472144/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649696583.472144/pipeline.pb in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-4d2661b4-f763-4cef-a753-3c26c77f1a36.json']\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-4d2661b4-f763-4cef-a753-3c26c77f1a36.json']\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20220411170303472987-2930'\n",
      " createTime: '2022-04-11T17:03:05.660668Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2022-04-11_10_03_05-4432069456366846862'\n",
      " location: 'us-central1'\n",
      " name: 'mgwaterdemobatch'\n",
      " projectId: 'mg-ce-demos'\n",
      " stageStates: []\n",
      " startTime: '2022-04-11T17:03:05.660668Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2022-04-11_10_03_05-4432069456366846862]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2022-04-11_10_03_05-4432069456366846862\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2022-04-11_10_03_05-4432069456366846862?project=mg-ce-demos\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-04-11_10_03_05-4432069456366846862 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:05.326Z: JOB_MESSAGE_BASIC: Dataflow Runner V2 auto-enabled. Use --experiments=disable_runner_v2 to opt out.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:06.839Z: JOB_MESSAGE_DETAILED: Autoscaling is enabled for job 2022-04-11_10_03_05-4432069456366846862. The number of workers will be between 1 and 1000.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:06.960Z: JOB_MESSAGE_DETAILED: Autoscaling was automatically enabled for job 2022-04-11_10_03_05-4432069456366846862.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:10.257Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-b.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:10.422Z: JOB_MESSAGE_WARNING: The network default doesn't have rules that open TCP ports 12345-12346 for internal connection with other VMs. Only rules with a target tag 'dataflow' or empty target tags set apply. If you don't specify such a rule, any pipeline with more than one worker that shuffles data will hang. Causes: Firewall rules associated with your network don't open TCP ports 12345-12346 for Dataflow instances. If a firewall rule opens connection in these ports, ensure target tags aren't specified, or that the rule includes the tag 'dataflow'.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:10.862Z: JOB_MESSAGE_DETAILED: Expanding SplittableParDo operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:10.912Z: JOB_MESSAGE_DETAILED: Expanding CollectionToSingleton operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.011Z: JOB_MESSAGE_DETAILED: Expanding CoGroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.055Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.085Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write/BigQueryBatchFileLoads/GroupFilesByTableDestinations: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.132Z: JOB_MESSAGE_DEBUG: Combiner lifting skipped for step Write/BigQueryBatchFileLoads/GroupShardedRows: GroupByKey not followed by a combiner.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.218Z: JOB_MESSAGE_DETAILED: Expanding GroupByKey operations into optimizable parts.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.269Z: JOB_MESSAGE_DEBUG: Annotating graph with Autotuner information.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.335Z: JOB_MESSAGE_DETAILED: Fusing adjacent ParDo, Read, Write, and Flatten operations\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.395Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/CopyJobNamePrefix into Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.436Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/GenerateFilePrefix into Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.474Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/LoadJobNamePrefix into Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.520Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/SchemaModJobNamePrefix into Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.562Z: JOB_MESSAGE_DETAILED: Unzipping flatten ref_AppliedPTransform_Write-BigQueryBatchFileLoads-DestinationFilesUnion_33 for input ref_AppliedPTransform_Write-BigQueryBatchFileLoads-ParDo-WriteRecordsToFile-ParDo-WriteRecordsToFile_28.WrittenFiles\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.622Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of Write/BigQueryBatchFileLoads/IdentityWorkaround, through flatten Write/BigQueryBatchFileLoads/DestinationFilesUnion, into producer Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.653Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles) into Write/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.687Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs) into Write/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.720Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables into Write/BigQueryBatchFileLoads/ParDo(PartitionFiles)/ParDo(PartitionFiles)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.763Z: JOB_MESSAGE_DETAILED: Unzipping flatten ref_AppliedPTransform_Write-BigQueryBatchFileLoads-DestinationFilesUnion_33-u26 for input ref_AppliedPTransform_Write-BigQueryBatchFileLoads-IdentityWorkaround_34.None-c24\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.805Z: JOB_MESSAGE_DETAILED: Fusing unzipped copy of Write/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write, through flatten Write/BigQueryBatchFileLoads/DestinationFilesUnion/Unzipped-1, into producer Write/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.836Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema) into Write/BigQueryBatchFileLoads/WaitForTempTableLoadJobs\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.870Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/IdentityWorkaround into Write/BigQueryBatchFileLoads/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.903Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write into Write/BigQueryBatchFileLoads/IdentityWorkaround\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.940Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/FlatMap(<lambda at core.py:3228>) into Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:11.977Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode) into Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/FlatMap(<lambda at core.py:3228>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.012Z: JOB_MESSAGE_DETAILED: Fusing consumer ReadFromText/Read/Map(<lambda at iobase.py:898>) into ReadFromText/Read/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.045Z: JOB_MESSAGE_DETAILED: Fusing consumer ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/PairWithRestriction into ReadFromText/Read/Map(<lambda at iobase.py:898>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.080Z: JOB_MESSAGE_DETAILED: Fusing consumer ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/SplitWithSizing into ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/PairWithRestriction\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.112Z: JOB_MESSAGE_DETAILED: Fusing consumer Map(parse_json) into ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/ProcessElementAndRestrictionWithSizing\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.145Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/RewindowIntoGlobal into Map(parse_json)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.219Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/AppendDestination into Write/BigQueryBatchFileLoads/RewindowIntoGlobal\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.249Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile) into Write/BigQueryBatchFileLoads/AppendDestination\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.280Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ParDo(_ShardDestinations) into Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.313Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/GroupShardedRows/Write into Write/BigQueryBatchFileLoads/ParDo(_ShardDestinations)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.349Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/DropShardNumber into Write/BigQueryBatchFileLoads/GroupShardedRows/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.383Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/WriteGroupedRecordsToFile into Write/BigQueryBatchFileLoads/DropShardNumber\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.428Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/FlatMap(<lambda at core.py:3228>) into Write/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.474Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Map(decode) into Write/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/FlatMap(<lambda at core.py:3228>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.520Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/WaitForTempTableLoadJobs into Write/BigQueryBatchFileLoads/ImpulseMonitorLoadJobs/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.561Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/FlatMap(<lambda at core.py:3228>) into Write/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.603Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Map(decode) into Write/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/FlatMap(<lambda at core.py:3228>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.635Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/WaitForSchemaModJobs into Write/BigQueryBatchFileLoads/ImpulseMonitorSchemaModJobs/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.672Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/FlatMap(<lambda at core.py:3228>) into Write/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.714Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Map(decode) into Write/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/FlatMap(<lambda at core.py:3228>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.749Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/WaitForCopyJobs into Write/BigQueryBatchFileLoads/ImpulseMonitorCopyJobs/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.782Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseEmptyPC/FlatMap(<lambda at core.py:3228>) into Write/BigQueryBatchFileLoads/ImpulseEmptyPC/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.816Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseEmptyPC/Map(decode) into Write/BigQueryBatchFileLoads/ImpulseEmptyPC/FlatMap(<lambda at core.py:3228>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.847Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/FlatMap(<lambda at core.py:3228>) into Write/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.880Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Map(decode) into Write/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/FlatMap(<lambda at core.py:3228>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.921Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/WaitForDestinationLoadJobs into Write/BigQueryBatchFileLoads/ImpulseMonitorDestinationLoadJobs/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.957Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/RemoveTempTables/Impulse/FlatMap(<lambda at core.py:3228>) into Write/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Impulse\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:12.988Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Map(decode) into Write/BigQueryBatchFileLoads/RemoveTempTables/Impulse/FlatMap(<lambda at core.py:3228>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.075Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/RemoveTempTables/PassTables into Write/BigQueryBatchFileLoads/RemoveTempTables/Impulse/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.109Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue into Write/BigQueryBatchFileLoads/RemoveTempTables/PassTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.142Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Write into Write/BigQueryBatchFileLoads/RemoveTempTables/AddUselessValue\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.205Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys into Write/BigQueryBatchFileLoads/RemoveTempTables/DeduplicateTables/Read\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.241Z: JOB_MESSAGE_DETAILED: Fusing consumer Write/BigQueryBatchFileLoads/RemoveTempTables/Delete into Write/BigQueryBatchFileLoads/RemoveTempTables/GetTableNames/Keys\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.282Z: JOB_MESSAGE_DEBUG: Workflow config is missing a default resource spec.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.317Z: JOB_MESSAGE_DEBUG: Adding StepResource setup and teardown to workflow graph.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.349Z: JOB_MESSAGE_DEBUG: Adding workflow start and stop steps.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.411Z: JOB_MESSAGE_DEBUG: Assigning stage ids.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.636Z: JOB_MESSAGE_DEBUG: Executing wait step start44\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.721Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/ImpulseEmptyPC/Impulse+Write/BigQueryBatchFileLoads/ImpulseEmptyPC/FlatMap(<lambda at core.py:3228>)+Write/BigQueryBatchFileLoads/ImpulseEmptyPC/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.754Z: JOB_MESSAGE_BASIC: Executing operation ReadFromText/Read/Impulse+ReadFromText/Read/Map(<lambda at iobase.py:898>)+ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/PairWithRestriction+ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/SplitWithSizing\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.768Z: JOB_MESSAGE_DEBUG: Starting worker pool setup.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.788Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Impulse+Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/FlatMap(<lambda at core.py:3228>)+Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)+Write/BigQueryBatchFileLoads/CopyJobNamePrefix+Write/BigQueryBatchFileLoads/GenerateFilePrefix+Write/BigQueryBatchFileLoads/LoadJobNamePrefix+Write/BigQueryBatchFileLoads/SchemaModJobNamePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:13.810Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1-b...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-04-11_10_03_05-4432069456366846862 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:03:49.646Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 1 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:04:25.030Z: JOB_MESSAGE_DETAILED: Workers have started successfully.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:16.794Z: JOB_MESSAGE_BASIC: Finished operation ReadFromText/Read/Impulse+ReadFromText/Read/Map(<lambda at iobase.py:898>)+ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/PairWithRestriction+ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/SplitWithSizing\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:16.862Z: JOB_MESSAGE_DEBUG: Value \"ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7-split-with-sizing-out3\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:19.698Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/ImpulseEmptyPC/Impulse+Write/BigQueryBatchFileLoads/ImpulseEmptyPC/FlatMap(<lambda at core.py:3228>)+Write/BigQueryBatchFileLoads/ImpulseEmptyPC/Map(decode)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:23.877Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Impulse+Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/FlatMap(<lambda at core.py:3228>)+Write/BigQueryBatchFileLoads/ImpulseSingleElementPC/Map(decode)+Write/BigQueryBatchFileLoads/CopyJobNamePrefix+Write/BigQueryBatchFileLoads/GenerateFilePrefix+Write/BigQueryBatchFileLoads/LoadJobNamePrefix+Write/BigQueryBatchFileLoads/SchemaModJobNamePrefix\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.520Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/CopyJobNamePrefix.None\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.577Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/GenerateFilePrefix.None\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.609Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/LoadJobNamePrefix.None\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.645Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/SchemaModJobNamePrefix.None\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.676Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.697Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/View-python_side_input0-Write/BigQueryBatchFileLoads/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.726Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.740Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/View-python_side_input0-Write/BigQueryBatchFileLoads/WriteGroupedRecordsToFile\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.750Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.782Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0-Write/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.805Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.814Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/View-python_side_input0-Write/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.838Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0-Write/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.846Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.867Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/View-python_side_input0-Write/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.868Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(TriggerCopyJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.892Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.906Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/WriteGroupedRecordsToFile/View-python_side_input0-Write/BigQueryBatchFileLoads/WriteGroupedRecordsToFile.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.938Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:24.970Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs)/View-python_side_input0-Write/BigQueryBatchFileLoads/TriggerLoadJobsWithTempTables/ParDo(TriggerLoadJobs).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:25.073Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables/View-python_side_input0-Write/BigQueryBatchFileLoads/TriggerLoadJobsWithoutTempTables.out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:25.108Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema)/View-python_side_input0-Write/BigQueryBatchFileLoads/ParDo(UpdateDestinationSchema).out\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:25.141Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:27.369Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/GroupShardedRows/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:27.439Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/GroupShardedRows/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:27.510Z: JOB_MESSAGE_BASIC: Executing operation Write/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:30.139Z: JOB_MESSAGE_BASIC: Finished operation Write/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Create\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:30.204Z: JOB_MESSAGE_DEBUG: Value \"Write/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Session\" materialized.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:30.305Z: JOB_MESSAGE_BASIC: Executing operation ref_AppliedPTransform_ReadFromText-Read-SDFBoundedSourceReader-ParDo-SDFBoundedSourceDoFn-_7/ProcessElementAndRestrictionWithSizing+Map(parse_json)+Write/BigQueryBatchFileLoads/RewindowIntoGlobal+Write/BigQueryBatchFileLoads/AppendDestination+Write/BigQueryBatchFileLoads/ParDo(WriteRecordsToFile)/ParDo(WriteRecordsToFile)+Write/BigQueryBatchFileLoads/IdentityWorkaround+Write/BigQueryBatchFileLoads/GroupFilesByTableDestinations/Write+Write/BigQueryBatchFileLoads/ParDo(_ShardDestinations)+Write/BigQueryBatchFileLoads/GroupShardedRows/Write\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:42.528Z: JOB_MESSAGE_DETAILED: Autoscaling: Resizing worker pool from 1 to 2.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:07:58.136Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 2 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:08:50.715Z: JOB_MESSAGE_DETAILED: Autoscaling: Resizing worker pool from 2 to 16.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:08:56.030Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 11 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:08:56.054Z: JOB_MESSAGE_DETAILED: Resized worker pool to 11, though goal was 16.  This could be a quota issue.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:09:06.539Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 15 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:09:06.571Z: JOB_MESSAGE_DETAILED: Resized worker pool to 15, though goal was 16.  This could be a quota issue.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:09:14.236Z: JOB_MESSAGE_WARNING: The network default doesn't have rules that open TCP ports 12345-12346 for internal connection with other VMs. Only rules with a target tag 'dataflow' or empty target tags set apply. If you don't specify such a rule, any pipeline with more than one worker that shuffles data will hang. Causes: Firewall rules associated with your network don't open TCP ports 12345-12346 for Dataflow instances. If a firewall rule opens connection in these ports, ensure target tags aren't specified, or that the rule includes the tag 'dataflow'.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:09:16.924Z: JOB_MESSAGE_DETAILED: Autoscaling: Raised the number of workers to 16 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:14:20.126Z: JOB_MESSAGE_DETAILED: Autoscaling: Reduced the number of workers to 15 based on the rate of progress in the currently running stage(s).\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:17:07.358Z: JOB_MESSAGE_DETAILED: Autoscaling: Resized worker pool from 13 to 0.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:17:07.491Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2022-04-11T17:17:07.550Z: JOB_MESSAGE_DEBUG: Tearing down pending resources...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2022-04-11_10_03_05-4432069456366846862 is in state JOB_STATE_DONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.43 s, sys: 331 ms, total: 4.76 s\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Execute\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dcd4f-c6bc-4666-a7a2-b98adfe6f5d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataflow Stream - Pub/Sub Topic to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7a6fb4-fd0f-48b7-8974-5ecde0100b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 1 µs, total: 9 µs\n",
      "Wall time: 15.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# function(s)\n",
    "\n",
    "def parse_json(line):\n",
    "    import json\n",
    "    record = json.loads(line)\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_stream(argv=None):\n",
    "    \"\"\"Build and run the pipeline.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd_mm_YY_H_M_S_ssssss - in this format so it can be used to create a unique BQ table\n",
    "    dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "    dt_string\n",
    "    \n",
    "    SCHEMA = 'radioId:STRING,eventTime:DATETIME,reading:FLOAT,consumptionFlag:STRING,reverseFlowFlag:STRING,noFlowFlag:STRING,emptyPipeFlag:STRING,temperature:FLOAT,pressure:FLOAT,batteryStatusFlag:STRING'\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runner', required=False, default='DataflowRunner', help='Dataflow Runner - DataflowRunner or DirectRunner (local)')\n",
    "    parser.add_argument('--job_name', required=False, default='mgwaterdemostream', help='Dataflow Job Name')\n",
    "    parser.add_argument('--batch_size', required=False, default='100', help='Dataflow Batch Size')\n",
    "    parser.add_argument('--input_topic', required=False, default='projects/mg-ce-demos/topics/smart-water', help='projects/<project_id>/topics/<topic_name>')\n",
    "    parser.add_argument('--project_id', required=False, default='mg-ce-demos', help='GCP Project ID')\n",
    "    parser.add_argument('--region', required=False, default='us-central1', help='GCP region for execution')\n",
    "    parser.add_argument('--dataset_name', required=False, default='smart_water_demo', help='Output BigQuery Dataset') \n",
    "    parser.add_argument('--table_name', required=False, default='smart_water_demo_data_stream_'+str(dt_string), help='Output BigQuery Table')\n",
    "    parser.add_argument('--input_data', required=False, default='gs://mg-ce-demos-bucket/water_data_stream_demo/output/*.json', help='input data (for batch only')\n",
    "\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    pipeline_args.extend([\n",
    "          '--runner=' + str(known_args.runner) # Change this to DataflowRunner to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--project=' + str(known_args.project_id) # Your project ID is required in order to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--staging_location=gs://mg-ce-demos-bucket/water_data_stream_demo/temp' # Your Google Cloud Storage path is required for staging local files.\n",
    "          ,'--temp_location=gs://mg-ce-demos-bucket/water_data_stream_demo/tmp' # Your Google Cloud Storage path is required for temporary files.\n",
    "          ,'--job_name=' + str(known_args.job_name) # Set project unique job name\n",
    "          ,'--region=' + str(known_args.region) # Set region if using DataflowRunner\n",
    "      ])\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    pipeline_options.view_as(StandardOptions).streaming = True  # set to True if stream (remove if batch)\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        \n",
    "        # Read the pubsub topic into a PCollection.\n",
    "        events = ( p | beam.io.ReadStringsFromPubSub(known_args.input_topic) )\n",
    "\n",
    "        # Tranform events\n",
    "        transformed = (events | beam.Map(parse_json))\n",
    "\n",
    "        # Persist to BigQuery\n",
    "        transformed | 'Write' >> beam.io.WriteToBigQuery(\n",
    "                            table=known_args.table_name,\n",
    "                            dataset=known_args.dataset_name,\n",
    "                            project=known_args.project_id,\n",
    "                            schema=SCHEMA,\n",
    "                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                            batch_size=int(known_args.batch_size)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921ba584-d3cc-4958-b8c3-f3ddd95b331e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/apache-beam-2.37.0/lib/python3.7/site-packages/ipykernel_launcher.py:50: BeamDeprecationWarning: ReadStringsFromPubSub is deprecated since 2.7.0. Use ReadFromPubSub instead.\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/root/apache-beam-2.37.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpb719p98z', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Full traceback: Traceback (most recent call last):\n  File \"/root/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/utils/processes.py\", line 89, in check_output\n    out = subprocess.check_output(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 411, in check_output\n    **kwargs).stdout\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 512, in run\n    output=stdout, stderr=stderr)\nsubprocess.CalledProcessError: Command '['/root/apache-beam-2.37.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpb719p98z', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']' returned non-zero exit status 1.\n \n Pip install failed for package: apache-beam==2.37.0           \n Output from execution of subprocess: b''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/utils/processes.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 411\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m             raise CalledProcessError(retcode, process.args,\n\u001b[0;32m--> 512\u001b[0;31m                                      output=stdout, stderr=stderr)\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['/root/apache-beam-2.37.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpb719p98z', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mrun_stream\u001b[0;34m(argv)\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             self._options).run(False)\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m       if (self._options.view_as(TypeOptions).runtime_type_check and\n",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m           \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_in_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    464\u001b[0m           environments.DockerEnvironment.from_container_image(\n\u001b[1;32m    465\u001b[0m               \u001b[0mapiclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_container_image_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m               \u001b[0martifacts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_sdk_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m               resource_hints=environments.resource_hints_from_options(options)))\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/transforms/environments.py\u001b[0m in \u001b[0;36mpython_sdk_dependencies\u001b[0;34m(options, tmp_dir)\u001b[0m\n\u001b[1;32m    808\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0martifact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPyPIArtifactRegistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       ],\n\u001b[0;32m--> 810\u001b[0;31m       skip_prestaged_dependencies=skip_prestaged_dependencies)\n\u001b[0m",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/runners/portability/stager.py\u001b[0m in \u001b[0;36mcreate_job_resources\u001b[0;34m(options, temp_dir, build_setup_args, pypi_requirements, populate_requirements_cache, skip_prestaged_dependencies)\u001b[0m\n\u001b[1;32m    288\u001b[0m           ) else setup_options.sdk_location\n\u001b[1;32m    289\u001b[0m           resources.extend(\n\u001b[0;32m--> 290\u001b[0;31m               Stager._create_beam_sdk(sdk_remote_location, temp_dir))\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msetup_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m           \u001b[0;31m# Use the SDK that's built into the container, rather than re-staging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/runners/portability/stager.py\u001b[0m in \u001b[0;36m_create_beam_sdk\u001b[0;34m(sdk_remote_location, temp_dir)\u001b[0m\n\u001b[1;32m    763\u001b[0m       \"\"\"\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msdk_remote_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pypi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0msdk_local_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_pypi_sdk_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m       \u001b[0msdk_sources_staged_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m           \u001b[0m_desired_sdk_filename_in_staging_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdk_local_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/runners/portability/stager.py\u001b[0m in \u001b[0;36m_download_pypi_sdk_package\u001b[0;34m(temp_dir, fetch_binary, language_version_tag, language_implementation_tag, abi_tag, platform_tag)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Executing command: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m       \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/utils/processes.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m           \u001b[0;31m\"\u001b[0m\u001b[0mFull\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0mn\u001b[0m \u001b[0mPip\u001b[0m \u001b[0minstall\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m           \u001b[0;31m\\\u001b[0m\u001b[0mn\u001b[0m \u001b[0mOutput\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m           .format(traceback.format_exc(), args[0][6], error.output))\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         raise RuntimeError(\"Full trace: {}, \\\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Full traceback: Traceback (most recent call last):\n  File \"/root/apache-beam-2.37.0/lib/python3.7/site-packages/apache_beam/utils/processes.py\", line 89, in check_output\n    out = subprocess.check_output(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 411, in check_output\n    **kwargs).stdout\n  File \"/opt/conda/lib/python3.7/subprocess.py\", line 512, in run\n    output=stdout, stderr=stderr)\nsubprocess.CalledProcessError: Command '['/root/apache-beam-2.37.0/bin/python', '-m', 'pip', 'download', '--dest', '/tmp/tmpb719p98z', 'apache-beam==2.37.0', '--no-deps', '--no-binary', ':all:']' returned non-zero exit status 1.\n \n Pip install failed for package: apache-beam==2.37.0           \n Output from execution of subprocess: b''"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Execute\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327844ae-6855-49a0-88d8-1c5af382aa5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9cdab-1e74-4341-8aea-d6e6ec5d5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do BQ schema - more details\n",
    "'''\n",
    "    table_schema = bigquery.TableSchema()\n",
    "    \n",
    "    # Fields that use standard types.\n",
    "    eventId_schema = bigquery.TableFieldSchema()\n",
    "    eventId_schema.name = 'eventId'\n",
    "    eventId_schema.type = 'string'\n",
    "    eventId_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(eventId_schema)\n",
    "    \n",
    "    deviceId_schema = bigquery.TableFieldSchema()\n",
    "    deviceId_schema.name = 'deviceId'\n",
    "    deviceId_schema.type = 'string'\n",
    "    deviceId_schema.mode = 'required'\n",
    "    table_schema.fields.append(deviceId_schema)\n",
    "    \n",
    "    eventTime = bigquery.TableFieldSchema()\n",
    "    eventTime.name = 'eventTime'\n",
    "    eventTime.type = 'datetime'\n",
    "    eventTime.mode = 'nullable'\n",
    "    table_schema.fields.append(eventTime)\n",
    "    \n",
    "    city_schema = bigquery.TableFieldSchema()\n",
    "    city_schema.name = 'city'\n",
    "    city_schema.type = 'string'\n",
    "    city_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(city_schema)\n",
    "    \n",
    "    temp_schema = bigquery.TableFieldSchema()\n",
    "    temp_schema.name = 'temp'\n",
    "    temp_schema.type = 'float'\n",
    "    temp_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(temp_schema)\n",
    "    \n",
    "    flowrate_schema = bigquery.TableFieldSchema()\n",
    "    flowrate_schema.name = 'flowrate'\n",
    "    flowrate_schema.type = 'float'\n",
    "    flowrate_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(flowrate_schema)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.37.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.37.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
