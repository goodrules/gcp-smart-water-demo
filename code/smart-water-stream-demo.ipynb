{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ef77e0-4160-451e-90cf-2d415d7634a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import apache_beam as beam\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "import google.auth\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ebfd3-cc0d-4b30-9016-5f831d00afe5",
   "metadata": {},
   "source": [
    "Prerequisites prior to running demo:\n",
    "1) Billing-enabled project\n",
    "2) API's enabled and necessary IAM role(s) for necessary services (e.g. Dataflow, BigQuery)\n",
    "3) GCS bucket for temporary and schema files\n",
    "4) BigQuery dataset (tables can / will be created if they don't already exist)\n",
    "5) Pub/Sub topic for streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8af55b-b72e-437c-8fa0-e4886d2a2a97",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataflow Batch - GCS Text Files to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b75dd2-0d94-475a-a755-de4b089afc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# function(s)\n",
    "\n",
    "def parse_json(line):\n",
    "    import json\n",
    "    record = json.loads(line)\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_batch(argv=None):\n",
    "    \"\"\"Build and run the pipeline.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd_mm_YY_H_M_S_ssssss - in this format so it can be used to create a unique BQ table\n",
    "    dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "    dt_string\n",
    "\n",
    "    SCHEMA = 'eventId:STRING,deviceId:STRING,eventTime:DATETIME,city:STRING,temp:FLOAT,flowrate:FLOAT'\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runner', required=False, default='DataflowRunner', help='Dataflow Runner - DataflowRunner or DirectRunner (local)')\n",
    "    parser.add_argument('--job_name', required=False, default='mgwaterdemobatch', help='Dataflow Job Name')\n",
    "    parser.add_argument('--project_id', required=False, default='mg-ce-demos', help='GCP Project ID')\n",
    "    parser.add_argument('--region', required=False, default='us-central1', help='GCP region for execution')\n",
    "    parser.add_argument('--dataset_name', required=False, default='smart_water_demo', help='Output BigQuery Dataset') \n",
    "    parser.add_argument('--table_name', required=False, default='smart_water_demo_data_batch_'+str(dt_string), help='Output BigQuery Table')\n",
    "    parser.add_argument('--input_data', required=False, default='gs://mg-ce-demos-bucket/water_data_stream_demo/output/*.json', help='input data (for batch only')\n",
    "\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    pipeline_args.extend([\n",
    "          '--runner=' + str(known_args.runner) # Change this to DataflowRunner to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--project=' + str(known_args.project_id) # Your project ID is required in order to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--staging_location=gs://mg-ce-demos-bucket/water_data_stream_demo/temp' # Your Google Cloud Storage path is required for staging local files.\n",
    "          ,'--temp_location=gs://mg-ce-demos-bucket/water_data_stream_demo/tmp' # Your Google Cloud Storage path is required for temporary files.\n",
    "          ,'--job_name=' + str(known_args.job_name) # Set project unique job name\n",
    "          ,'--region=' + str(known_args.region) # Set region if using DataflowRunner\n",
    "      ])\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    #pipeline_options.view_as(StandardOptions).streaming = True  # set to True if stream (remove if batch)\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        \n",
    "        # Read the JSON files in GCS into a PCollection.\n",
    "        events = ( p | beam.io.ReadFromText(known_args.input_data) )  #change to read files from GCS\n",
    "\n",
    "        # Tranform events\n",
    "        transformed = (events | beam.Map(parse_json))\n",
    "\n",
    "        # Persist to BigQuery\n",
    "        transformed | 'Write' >> beam.io.WriteToBigQuery(\n",
    "                            table=known_args.table_name,\n",
    "                            dataset=known_args.dataset_name,\n",
    "                            project=known_args.project_id,\n",
    "                            schema=SCHEMA,\n",
    "                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b1050c-f84e-45af-95fd-778d5ff64267",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.internal.gcp.auth:Setting socket default timeout to 60 seconds.\n",
      "INFO:apache_beam.internal.gcp.auth:socket default timeout is 60.0 seconds.\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 28 files in 0.28203511238098145 seconds.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikegoodman/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/io/gcp/bigquery.py:2106: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  is_streaming_pipeline = p.options.view_as(StandardOptions).streaming\n",
      "/Users/mikegoodman/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/io/gcp/bigquery_file_loads.py:1112: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  temp_location = p.options.view_as(GoogleCloudOptions).temp_location\n",
      "INFO:apache_beam.runners.portability.stager:Downloading source distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/Users/mikegoodman/Documents/developer/venv/bin/python', '-m', 'pip', 'download', '--dest', '/var/folders/q8/1gf829m141157nbz7kc4_19h00tb0q/T/tmpht3fz2_u', 'apache-beam==2.35.0', '--no-deps', '--no-binary', ':all:']\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/mikegoodman/Documents/developer/venv/bin/python -m pip install --upgrade pip' command.\n",
      "INFO:apache_beam.runners.portability.stager:Staging SDK sources from PyPI: dataflow_python_sdk.tar\n",
      "INFO:apache_beam.runners.portability.stager:Downloading binary distribution of the SDK from PyPi\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/Users/mikegoodman/Documents/developer/venv/bin/python', '-m', 'pip', 'download', '--dest', '/var/folders/q8/1gf829m141157nbz7kc4_19h00tb0q/T/tmpht3fz2_u', 'apache-beam==2.35.0', '--no-deps', '--only-binary', ':all:', '--python-version', '39', '--implementation', 'cp', '--abi', 'cp39', '--platform', 'manylinux1_x86_64']\n",
      "ERROR: Could not find a version that satisfies the requirement apache-beam==2.35.0 (from versions: 2.37.0rc3, 2.37.0)\n",
      "ERROR: No matching distribution found for apache-beam==2.35.0\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/Users/mikegoodman/Documents/developer/venv/bin/python -m pip install --upgrade pip' command.\n",
      "WARNING:apache_beam.runners.portability.stager:Failed to download requested binary distribution of the SDK: RuntimeError('Full traceback: Traceback (most recent call last):\\n  File \"/Users/mikegoodman/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/utils/processes.py\", line 89, in check_output\\n    out = subprocess.check_output(*args, **kwargs)\\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\", line 424, in check_output\\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\\n  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\", line 528, in run\\n    raise CalledProcessError(retcode, process.args,\\nsubprocess.CalledProcessError: Command \\'[\\'/Users/mikegoodman/Documents/developer/venv/bin/python\\', \\'-m\\', \\'pip\\', \\'download\\', \\'--dest\\', \\'/var/folders/q8/1gf829m141157nbz7kc4_19h00tb0q/T/tmpht3fz2_u\\', \\'apache-beam==2.35.0\\', \\'--no-deps\\', \\'--only-binary\\', \\':all:\\', \\'--python-version\\', \\'39\\', \\'--implementation\\', \\'cp\\', \\'--abi\\', \\'cp39\\', \\'--platform\\', \\'manylinux1_x86_64\\']\\' returned non-zero exit status 1.\\n \\n Pip install failed for package: apache-beam==2.35.0           \\n Output from execution of subprocess: b\\'\\'')\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.35.0\n",
      "INFO:root:Using provided Python SDK container image: gcr.io/cloud-dataflow/v1beta3/python39:2.35.0\n",
      "INFO:root:Python SDK container image set to \"gcr.io/cloud-dataflow/v1beta3/python39:2.35.0\" for Docker environment\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7fbc29b5d1f0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7fbc29b5d9d0> ====================\n",
      "INFO:apache_beam.io.gcp.gcsio:Starting the size estimation of the input\n",
      "INFO:apache_beam.io.gcp.gcsio:Finished listing 28 files in 0.11905097961425781 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649164132.167747/pickled_main_session...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649164132.167747/pickled_main_session in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649164132.167747/dataflow_python_sdk.tar...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649164132.167747/dataflow_python_sdk.tar in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649164132.167747/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://mg-ce-demos-bucket/water_data_stream_demo/temp/mgwaterdemobatch.1649164132.167747/pipeline.pb in 0 seconds.\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Dataflow runner currently supports Python versions ['3.6', '3.7', '3.8'], got 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53) \n[Clang 6.0 (clang-600.0.57)].\nTo ignore this requirement and start a job using an unsupported version of Python interpreter, pass --experiment use_unsupported_python_version pipeline option.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mrun_batch\u001b[0;34m(argv)\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    544\u001b[0m       \u001b[0;31m# When possible, invoke a round trip through the runner API.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtest_runner_api\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_runner_api_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m         return Pipeline.from_runner_api(\n\u001b[0m\u001b[1;32m    547\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m           \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_in_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;31m# raise an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     result = DataflowPipelineResult(\n\u001b[0;32m--> 581\u001b[0;31m         self.dataflow_client.create_job(self.job), self)\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;31m# TODO(BEAM-4274): Circular import runners-metrics. Requires refactoring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/utils/retry.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36mcreate_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    671\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;34m\"\"\"Creates job description. May stage and/or submit for remote execution.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;31m# Stage and submit the job when necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36mcreate_job_description\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    769\u001b[0m         io.BytesIO(job.proto_pipeline.SerializeToString()))\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     job.proto.environment = Environment(\n\u001b[0m\u001b[1;32m    772\u001b[0m         proto_pipeline_staged_url=FileSystems.join(\n\u001b[1;32m    773\u001b[0m             \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle_cloud_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstaging_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, packages, options, environment_version, proto_pipeline_staged_url, proto_pipeline, _sdk_image_overrides)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Version information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersionValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0m_verify_interpreter_version_is_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m       \u001b[0mjob_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'FNAPI_STREAMING'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/developer/venv/lib/python3.9/site-packages/apache_beam/runners/dataflow/internal/apiclient.py\u001b[0m in \u001b[0;36m_verify_interpreter_version_is_supported\u001b[0;34m(pipeline_options)\u001b[0m\n\u001b[1;32m   1219\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m   raise Exception(\n\u001b[0m\u001b[1;32m   1222\u001b[0m       \u001b[0;34m'Dataflow runner currently supports Python versions %s, got %s.\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       \u001b[0;34m'To ignore this requirement and start a job '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Dataflow runner currently supports Python versions ['3.6', '3.7', '3.8'], got 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53) \n[Clang 6.0 (clang-600.0.57)].\nTo ignore this requirement and start a job using an unsupported version of Python interpreter, pass --experiment use_unsupported_python_version pipeline option."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Execute\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dcd4f-c6bc-4666-a7a2-b98adfe6f5d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataflow Stream - Pub/Sub Topic to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a6fb4-fd0f-48b7-8974-5ecde0100b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# function(s)\n",
    "\n",
    "def parse_json(line):\n",
    "    import json\n",
    "    record = json.loads(line)\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_stream(argv=None):\n",
    "    \"\"\"Build and run the pipeline.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd_mm_YY_H_M_S_ssssss - in this format so it can be used to create a unique BQ table\n",
    "    dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "    dt_string\n",
    "    \n",
    "    SCHEMA = 'eventId:STRING,deviceId:STRING,eventTime:DATETIME,city:STRING,temp:FLOAT,flowrate:FLOAT'  # Simple BQ Schema\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runner', required=False, default='DataflowRunner', help='Dataflow Runner - DataflowRunner or DirectRunner (local)')\n",
    "    parser.add_argument('--job_name', required=False, default='mgwaterdemostream', help='Dataflow Job Name')\n",
    "    parser.add_argument('--batch_size', required=False, default='100', help='Dataflow Batch Size')\n",
    "    parser.add_argument('--input_topic', required=False, default='projects/mg-ce-demos/topics/smart-water', help='projects/<project_id>/topics/<topic_name>')\n",
    "    parser.add_argument('--project_id', required=False, default='mg-ce-demos', help='GCP Project ID')\n",
    "    parser.add_argument('--region', required=False, default='us-central1', help='GCP region for execution')\n",
    "    parser.add_argument('--dataset_name', required=False, default='smart_water_demo', help='Output BigQuery Dataset') \n",
    "    parser.add_argument('--table_name', required=False, default='smart_water_demo_data_stream_'+str(dt_string), help='Output BigQuery Table')\n",
    "    parser.add_argument('--input_data', required=False, default='gs://mg-ce-demos-bucket/water_data_stream_demo/output/*.json', help='input data (for batch only')\n",
    "\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    pipeline_args.extend([\n",
    "          '--runner=' + str(known_args.runner) # Change this to DataflowRunner to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--project=' + str(known_args.project_id) # Your project ID is required in order to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--staging_location=gs://mg-ce-demos-bucket/water_data_stream_demo/temp' # Your Google Cloud Storage path is required for staging local files.\n",
    "          ,'--temp_location=gs://mg-ce-demos-bucket/water_data_stream_demo/tmp' # Your Google Cloud Storage path is required for temporary files.\n",
    "          ,'--job_name=' + str(known_args.job_name) # Set project unique job name\n",
    "          ,'--region=' + str(known_args.region) # Set region if using DataflowRunner\n",
    "      ])\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    pipeline_options.view_as(StandardOptions).streaming = True  # set to True if stream (remove if batch)\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        \n",
    "        # Read the pubsub topic into a PCollection.\n",
    "        events = ( p | beam.io.ReadStringsFromPubSub(known_args.input_topic) )\n",
    "\n",
    "        # Tranform events\n",
    "        transformed = (events | beam.Map(parse_json))\n",
    "\n",
    "        # Persist to BigQuery\n",
    "        transformed | 'Write' >> beam.io.WriteToBigQuery(\n",
    "                            table=known_args.table_name,\n",
    "                            dataset=known_args.dataset_name,\n",
    "                            project=known_args.project_id,\n",
    "                            schema=SCHEMA,\n",
    "                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                            batch_size=int(known_args.batch_size)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ba584-d3cc-4958-b8c3-f3ddd95b331e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Execute\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327844ae-6855-49a0-88d8-1c5af382aa5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9cdab-1e74-4341-8aea-d6e6ec5d5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do BQ schema - more details\n",
    "'''\n",
    "    table_schema = bigquery.TableSchema()\n",
    "    \n",
    "    # Fields that use standard types.\n",
    "    eventId_schema = bigquery.TableFieldSchema()\n",
    "    eventId_schema.name = 'eventId'\n",
    "    eventId_schema.type = 'string'\n",
    "    eventId_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(eventId_schema)\n",
    "    \n",
    "    deviceId_schema = bigquery.TableFieldSchema()\n",
    "    deviceId_schema.name = 'deviceId'\n",
    "    deviceId_schema.type = 'string'\n",
    "    deviceId_schema.mode = 'required'\n",
    "    table_schema.fields.append(deviceId_schema)\n",
    "    \n",
    "    eventTime = bigquery.TableFieldSchema()\n",
    "    eventTime.name = 'eventTime'\n",
    "    eventTime.type = 'datetime'\n",
    "    eventTime.mode = 'nullable'\n",
    "    table_schema.fields.append(eventTime)\n",
    "    \n",
    "    city_schema = bigquery.TableFieldSchema()\n",
    "    city_schema.name = 'city'\n",
    "    city_schema.type = 'string'\n",
    "    city_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(city_schema)\n",
    "    \n",
    "    temp_schema = bigquery.TableFieldSchema()\n",
    "    temp_schema.name = 'temp'\n",
    "    temp_schema.type = 'float'\n",
    "    temp_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(temp_schema)\n",
    "    \n",
    "    flowrate_schema = bigquery.TableFieldSchema()\n",
    "    flowrate_schema.name = 'flowrate'\n",
    "    flowrate_schema.type = 'float'\n",
    "    flowrate_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(flowrate_schema)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
