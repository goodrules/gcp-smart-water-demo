{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ef77e0-4160-451e-90cf-2d415d7634a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import apache_beam as beam\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "import google.auth\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ebfd3-cc0d-4b30-9016-5f831d00afe5",
   "metadata": {},
   "source": [
    "Prerequisites prior to running demo:\n",
    "1) Billing-enabled project\n",
    "2) API's enabled and necessary IAM role(s) for necessary services (e.g. Dataflow, BigQuery)\n",
    "3) GCS bucket for temporary and schema files\n",
    "4) BigQuery dataset (tables can / will be created if they don't already exist)\n",
    "5) Pub/Sub topic for streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8af55b-b72e-437c-8fa0-e4886d2a2a97",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataflow Batch - GCS Text Files to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b75dd2-0d94-475a-a755-de4b089afc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 3 µs, total: 11 µs\n",
      "Wall time: 35 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# function(s)\n",
    "\n",
    "def parse_json(line):\n",
    "    import json\n",
    "    record = json.loads(line)\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_batch(argv=None):\n",
    "    \"\"\"Build and run the pipeline.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd_mm_YY_H_M_S_ssssss - in this format so it can be used to create a unique BQ table\n",
    "    dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "    dt_string\n",
    "\n",
    "    SCHEMA = 'eventId:STRING,deviceId:STRING,eventTime:DATETIME,city:STRING,temp:FLOAT,flowrate:FLOAT'\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runner', required=False, default='DataflowRunner', help='Dataflow Runner - DataflowRunner or DirectRunner (local)')\n",
    "    parser.add_argument('--job_name', required=False, default='mgwaterdemobatch', help='Dataflow Job Name')\n",
    "    parser.add_argument('--project_id', required=False, default='mg-ce-demos', help='GCP Project ID')\n",
    "    parser.add_argument('--region', required=False, default='us-central1', help='GCP region for execution')\n",
    "    parser.add_argument('--dataset_name', required=False, default='smart_water_demo', help='Output BigQuery Dataset') \n",
    "    parser.add_argument('--table_name', required=False, default='smart_water_demo_data_batch_'+str(dt_string), help='Output BigQuery Table')\n",
    "    parser.add_argument('--input_data', required=False, default='gs://mg-ce-demos-bucket/water_data_stream_demo/output_old/*.json', help='input data (for batch only')\n",
    "\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    pipeline_args.extend([\n",
    "          '--runner=' + str(known_args.runner) # Change this to DataflowRunner to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--project=' + str(known_args.project_id) # Your project ID is required in order to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--staging_location=gs://mg-ce-demos-bucket/water_data_stream_demo/temp' # Your Google Cloud Storage path is required for staging local files.\n",
    "          ,'--temp_location=gs://mg-ce-demos-bucket/water_data_stream_demo/tmp' # Your Google Cloud Storage path is required for temporary files.\n",
    "          ,'--job_name=' + str(known_args.job_name) # Set project unique job name\n",
    "          ,'--region=' + str(known_args.region) # Set region if using DataflowRunner\n",
    "      ])\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    #pipeline_options.view_as(StandardOptions).streaming = True  # set to True if stream (remove if batch)\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        \n",
    "        # Read the JSON files in GCS into a PCollection.\n",
    "        events = ( p | beam.io.ReadFromText(known_args.input_data) )  #change to read files from GCS\n",
    "\n",
    "        # Tranform events\n",
    "        transformed = (events | beam.Map(parse_json))\n",
    "\n",
    "        # Persist to BigQuery\n",
    "        transformed | 'Write' >> beam.io.WriteToBigQuery(\n",
    "                            table=known_args.table_name,\n",
    "                            dataset=known_args.dataset_name,\n",
    "                            project=known_args.project_id,\n",
    "                            schema=SCHEMA,\n",
    "                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b1050c-f84e-45af-95fd-778d5ff64267",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to get filesystem from specified path, please use the correct path or ensure the required dependency is installed, e.g., pip install apache-beam[gcp]. Path specified: gs://mg-ce-demos-bucket/water_data_stream_demo/output_old/*.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:48\u001b[0m, in \u001b[0;36mrun_batch\u001b[0;34m(argv)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/developer/venv38/lib/python3.8/site-packages/apache_beam/io/textio.py:666\u001b[0m, in \u001b[0;36mReadFromText.__init__\u001b[0;34m(self, file_pattern, min_bundle_size, compression_type, strip_trailing_newlines, coder, validate, skip_header_lines, delimiter, escapechar, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m\"\"\"Initialize the :class:`ReadFromText` transform.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    delimiter, can also escape itself.\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_source_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_bundle_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrip_trailing_newlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_header_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_header_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/developer/venv38/lib/python3.8/site-packages/apache_beam/io/textio.py:128\u001b[0m, in \u001b[0;36m_TextSource.__init__\u001b[0;34m(self, file_pattern, min_bundle_size, compression_type, strip_trailing_newlines, coder, buffer_size, validate, skip_header_lines, header_processor_fns, delimiter, escapechar)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     94\u001b[0m              file_pattern,\n\u001b[1;32m     95\u001b[0m              min_bundle_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m              delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m              escapechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    105\u001b[0m   \u001b[38;5;124;03m\"\"\"Initialize a _TextSource\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m  of the arguments.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m   \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfile_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmin_bundle_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompression_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_trailing_newlines \u001b[38;5;241m=\u001b[39m strip_trailing_newlines\n\u001b[1;32m    135\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression_type \u001b[38;5;241m=\u001b[39m compression_type\n",
      "File \u001b[0;32m~/Documents/developer/venv38/lib/python3.8/site-packages/apache_beam/io/filebasedsource.py:124\u001b[0m, in \u001b[0;36mFileBasedSource.__init__\u001b[0;34m(self, file_pattern, min_bundle_size, compression_type, splittable, validate)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_splittable \u001b[38;5;241m=\u001b[39m splittable\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mand\u001b[39;00m file_pattern\u001b[38;5;241m.\u001b[39mis_accessible():\n\u001b[0;32m--> 124\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/developer/venv38/lib/python3.8/site-packages/apache_beam/options/value_provider.py:193\u001b[0m, in \u001b[0;36mcheck_accessible.<locals>._check_accessible.<locals>._f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mis_accessible():\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mRuntimeValueProviderError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m not accessible\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m obj)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfnc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/developer/venv38/lib/python3.8/site-packages/apache_beam/io/filebasedsource.py:185\u001b[0m, in \u001b[0;36mFileBasedSource._validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pattern\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# Limit the responses as we only want to check if something exists\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m match_result \u001b[38;5;241m=\u001b[39m \u001b[43mFileSystems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(match_result\u001b[38;5;241m.\u001b[39mmetadata_list) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    187\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo files found based on the file pattern \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m pattern)\n",
      "File \u001b[0;32m~/Documents/developer/venv38/lib/python3.8/site-packages/apache_beam/io/filesystems.py:203\u001b[0m, in \u001b[0;36mFileSystems.match\u001b[0;34m(patterns, limits)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(patterns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    202\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 203\u001b[0m filesystem \u001b[38;5;241m=\u001b[39m \u001b[43mFileSystems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_filesystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem\u001b[38;5;241m.\u001b[39mmatch(patterns, limits)\n",
      "File \u001b[0;32m~/Documents/developer/venv38/lib/python3.8/site-packages/apache_beam/io/filesystems.py:103\u001b[0m, in \u001b[0;36mFileSystems.get_filesystem\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     98\u001b[0m systems \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     99\u001b[0m     fs \u001b[38;5;28;01mfor\u001b[39;00m fs \u001b[38;5;129;01min\u001b[39;00m FileSystem\u001b[38;5;241m.\u001b[39mget_all_subclasses()\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mscheme() \u001b[38;5;241m==\u001b[39m path_scheme\n\u001b[1;32m    101\u001b[0m ]\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(systems) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to get filesystem from specified path, please use the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    105\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrect path or ensure the required dependency is installed, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    106\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124me.g., pip install apache-beam[gcp]. Path specified: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m path)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(systems) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# Pipeline options could come either from the Pipeline itself (using\u001b[39;00m\n\u001b[1;32m    109\u001b[0m   \u001b[38;5;66;03m# direct runner), or via RuntimeValueProvider (other runners).\u001b[39;00m\n\u001b[1;32m    110\u001b[0m   options \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m       FileSystems\u001b[38;5;241m.\u001b[39m_pipeline_options \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    112\u001b[0m       RuntimeValueProvider\u001b[38;5;241m.\u001b[39mruntime_options)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to get filesystem from specified path, please use the correct path or ensure the required dependency is installed, e.g., pip install apache-beam[gcp]. Path specified: gs://mg-ce-demos-bucket/water_data_stream_demo/output_old/*.json"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Execute\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dcd4f-c6bc-4666-a7a2-b98adfe6f5d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataflow Stream - Pub/Sub Topic to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a6fb4-fd0f-48b7-8974-5ecde0100b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# function(s)\n",
    "\n",
    "def parse_json(line):\n",
    "    import json\n",
    "    record = json.loads(line)\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_stream(argv=None):\n",
    "    \"\"\"Build and run the pipeline.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd_mm_YY_H_M_S_ssssss - in this format so it can be used to create a unique BQ table\n",
    "    dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "    dt_string\n",
    "    \n",
    "    SCHEMA = 'eventId:STRING,deviceId:STRING,eventTime:DATETIME,city:STRING,temp:FLOAT,flowrate:FLOAT'  # Simple BQ Schema\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runner', required=False, default='DataflowRunner', help='Dataflow Runner - DataflowRunner or DirectRunner (local)')\n",
    "    parser.add_argument('--job_name', required=False, default='mgwaterdemostream', help='Dataflow Job Name')\n",
    "    parser.add_argument('--batch_size', required=False, default='100', help='Dataflow Batch Size')\n",
    "    parser.add_argument('--input_topic', required=False, default='projects/mg-ce-demos/topics/smart-water', help='projects/<project_id>/topics/<topic_name>')\n",
    "    parser.add_argument('--project_id', required=False, default='mg-ce-demos', help='GCP Project ID')\n",
    "    parser.add_argument('--region', required=False, default='us-central1', help='GCP region for execution')\n",
    "    parser.add_argument('--dataset_name', required=False, default='smart_water_demo', help='Output BigQuery Dataset') \n",
    "    parser.add_argument('--table_name', required=False, default='smart_water_demo_data_stream_'+str(dt_string), help='Output BigQuery Table')\n",
    "    parser.add_argument('--input_data', required=False, default='gs://mg-ce-demos-bucket/water_data_stream_demo/output/*.json', help='input data (for batch only')\n",
    "\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    pipeline_args.extend([\n",
    "          '--runner=' + str(known_args.runner) # Change this to DataflowRunner to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--project=' + str(known_args.project_id) # Your project ID is required in order to run your pipeline on the Google Cloud Dataflow Service.\n",
    "          ,'--staging_location=gs://mg-ce-demos-bucket/water_data_stream_demo/temp' # Your Google Cloud Storage path is required for staging local files.\n",
    "          ,'--temp_location=gs://mg-ce-demos-bucket/water_data_stream_demo/tmp' # Your Google Cloud Storage path is required for temporary files.\n",
    "          ,'--job_name=' + str(known_args.job_name) # Set project unique job name\n",
    "          ,'--region=' + str(known_args.region) # Set region if using DataflowRunner\n",
    "      ])\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    pipeline_options.view_as(StandardOptions).streaming = True  # set to True if stream (remove if batch)\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        \n",
    "        # Read the pubsub topic into a PCollection.\n",
    "        events = ( p | beam.io.ReadStringsFromPubSub(known_args.input_topic) )\n",
    "\n",
    "        # Tranform events\n",
    "        transformed = (events | beam.Map(parse_json))\n",
    "\n",
    "        # Persist to BigQuery\n",
    "        transformed | 'Write' >> beam.io.WriteToBigQuery(\n",
    "                            table=known_args.table_name,\n",
    "                            dataset=known_args.dataset_name,\n",
    "                            project=known_args.project_id,\n",
    "                            schema=SCHEMA,\n",
    "                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                            batch_size=int(known_args.batch_size)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ba584-d3cc-4958-b8c3-f3ddd95b331e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Execute\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327844ae-6855-49a0-88d8-1c5af382aa5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9cdab-1e74-4341-8aea-d6e6ec5d5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do BQ schema - more details\n",
    "'''\n",
    "    table_schema = bigquery.TableSchema()\n",
    "    \n",
    "    # Fields that use standard types.\n",
    "    eventId_schema = bigquery.TableFieldSchema()\n",
    "    eventId_schema.name = 'eventId'\n",
    "    eventId_schema.type = 'string'\n",
    "    eventId_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(eventId_schema)\n",
    "    \n",
    "    deviceId_schema = bigquery.TableFieldSchema()\n",
    "    deviceId_schema.name = 'deviceId'\n",
    "    deviceId_schema.type = 'string'\n",
    "    deviceId_schema.mode = 'required'\n",
    "    table_schema.fields.append(deviceId_schema)\n",
    "    \n",
    "    eventTime = bigquery.TableFieldSchema()\n",
    "    eventTime.name = 'eventTime'\n",
    "    eventTime.type = 'datetime'\n",
    "    eventTime.mode = 'nullable'\n",
    "    table_schema.fields.append(eventTime)\n",
    "    \n",
    "    city_schema = bigquery.TableFieldSchema()\n",
    "    city_schema.name = 'city'\n",
    "    city_schema.type = 'string'\n",
    "    city_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(city_schema)\n",
    "    \n",
    "    temp_schema = bigquery.TableFieldSchema()\n",
    "    temp_schema.name = 'temp'\n",
    "    temp_schema.type = 'float'\n",
    "    temp_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(temp_schema)\n",
    "    \n",
    "    flowrate_schema = bigquery.TableFieldSchema()\n",
    "    flowrate_schema.name = 'flowrate'\n",
    "    flowrate_schema.type = 'float'\n",
    "    flowrate_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(flowrate_schema)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
